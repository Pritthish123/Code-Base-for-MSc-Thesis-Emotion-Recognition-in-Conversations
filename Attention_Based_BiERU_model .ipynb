{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing of libraries\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n",
    "                        classification_report, precision_recall_fscore_support\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from dataloader import IEMOCAPDataset\n",
    "import torch.nn.functional as F\n",
    "from madgrad import MADGRAD \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data in train set, test set and validation set\n",
    "def get_train_valid_sampler(trainset, valid=0.1):\n",
    "    size = len(trainset)\n",
    "    idx = list(range(size))\n",
    "    split = int(valid*size)\n",
    "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
    "\n",
    "\n",
    "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
    "    trainset = IEMOCAPDataset(path=path)\n",
    "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
    "    train_loader = DataLoader(trainset,\n",
    "                              batch_size=batch_size,\n",
    "                              sampler=train_sampler,\n",
    "                              collate_fn=trainset.collate_fn,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "    valid_loader = DataLoader(trainset,\n",
    "                              batch_size=batch_size,\n",
    "                              sampler=valid_sampler,\n",
    "                              collate_fn=trainset.collate_fn,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "    testset = IEMOCAPDataset(path=path, train=False)\n",
    "    test_loader = DataLoader(testset,\n",
    "                             batch_size=batch_size,\n",
    "                             collate_fn=testset.collate_fn,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_memory=pin_memory)\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Attention Layer\n",
    "class SimpleAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.scalar = nn.Linear(self.input_dim,1,bias=False)\n",
    "\n",
    "    def forward(self, M, x=None):\n",
    "        \n",
    "        scale = self.scalar(M)\n",
    "        alpha = F.softmax(scale, dim=0).permute(1,2,0)\n",
    "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] \n",
    "        return attn_pool, alpha\n",
    "\n",
    "class MatchingAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type='general'):\n",
    "        super(MatchingAttention, self).__init__()\n",
    "        assert att_type!='concat' or alpha_dim!=None\n",
    "        assert att_type!='dot' or mem_dim==cand_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.cand_dim = cand_dim\n",
    "        self.att_type = att_type\n",
    "        if att_type=='general':\n",
    "            self.transform = nn.Linear(cand_dim, mem_dim, bias=False)\n",
    "        if att_type=='general2':\n",
    "            self.transform = nn.Linear(cand_dim, mem_dim, bias=True)\n",
    "        elif att_type=='concat':\n",
    "            self.transform = nn.Linear(cand_dim+mem_dim, alpha_dim, bias=False)\n",
    "            self.vector_prod = nn.Linear(alpha_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, M, x, mask=None):\n",
    "        \n",
    "        if type(mask)==type(None):\n",
    "            mask = torch.ones(M.size(1), M.size(0)).type(M.type())\n",
    "\n",
    "        if self.att_type=='dot':\n",
    "            \n",
    "            M_ = M.permute(1,2,0) \n",
    "            x_ = x.unsqueeze(1) \n",
    "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) \n",
    "        elif self.att_type=='general':\n",
    "            M_ = M.permute(1,2,0) \n",
    "            x_ = self.transform(x).unsqueeze(1) \n",
    "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) \n",
    "        elif self.att_type=='general2':\n",
    "            M_ = M.permute(1,2,0) \n",
    "            x_ = self.transform(x).unsqueeze(1) \n",
    "            alpha_ = F.softmax((torch.bmm(x_, M_))*mask.unsqueeze(1), dim=2) \n",
    "            alpha_masked = alpha_*mask.unsqueeze(1) \n",
    "            alpha_sum = torch.sum(alpha_masked, dim=2, keepdim=True) \n",
    "            alpha = alpha_masked/alpha_sum \n",
    "        else:\n",
    "            M_ = M.transpose(0,1) \n",
    "            x_ = x.unsqueeze(1).expand(-1,M.size()[0],-1) \n",
    "            M_x_ = torch.cat([M_,x_],2) \n",
    "            mx_a = F.tanh(self.transform(M_x_)) \n",
    "            alpha = F.softmax(self.vector_prod(mx_a),1).transpose(1,2) \n",
    "\n",
    "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] \n",
    "        return attn_pool, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining class of the Loss Function \n",
    "class MaskedNLLLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight=None):\n",
    "        super(MaskedNLLLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.loss = nn.NLLLoss(weight=weight,reduction='sum')\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        \n",
    "        mask_ = mask.view(-1, 1) # batch*seq_len, 1\n",
    "        if type(self.weight) == type(None):\n",
    "            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n",
    "        else:\n",
    "            loss = self.loss(pred*mask_, target)/torch.sum(self.weight[target]*mask_.squeeze())\n",
    "        return loss\n",
    "\n",
    "# Defining the model implemented and the all the experiemnts of the model is run on Laptop CPU\n",
    "class RNTN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, n_class,context_attention, l):\n",
    "        super(RNTN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_class = n_class\n",
    "        self.window = 5\n",
    "        context_attention='simple'\n",
    "        self.V = nn.Parameter(torch.zeros((input_dim, 1, 2*input_dim, 2*input_dim)))\n",
    "\n",
    "        self.W = nn.Linear(2*input_dim, input_dim)\n",
    "        \n",
    "        self.Ws = nn.Linear(2*(52 + input_dim), n_class)\n",
    "        self.gru = nn.LSTMCell(input_size=input_dim, hidden_size=input_dim)\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=5, padding=4, stride=2)\n",
    "        self.ac = nn.Sigmoid()\n",
    "        self.ac_linear = nn.ReLU()\n",
    "        self.ac_tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.l = l\n",
    "        self.cnn3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=5, padding=4, stride=2)\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.bilstm = nn.LSTM(input_size=input_dim, hidden_size=input_dim, bidirectional=True)\n",
    "        #if l:\n",
    "        #    self.L = nn.Linear(2*input_dim, input_dim)\n",
    "            \n",
    "        if context_attention=='simple':\n",
    "            self.attention = SimpleAttention(input_dim)\n",
    "        else:\n",
    "            self.attention = MatchingAttention(input_dim, n_class, context_attention,att_type)\n",
    "            \n",
    "    # X being the seq_len , batch  and dim\n",
    "    # Mask being batch,seq_len\n",
    "    def _reverse_seq(self, X, mask): \n",
    "        X_ = X.transpose(0,1)\n",
    "        mask_sum = torch.sum(mask, 1).int()\n",
    "\n",
    "        xfs = []\n",
    "        for x, c in zip(X_, mask_sum):\n",
    "            xf = torch.flip(x[:c], [0])\n",
    "            xfs.append(xf)\n",
    "\n",
    "        return pad_sequence(xfs)\n",
    "    \n",
    "    # Defining the forward class\n",
    "    # U being seq , batch and dim\n",
    "    def forward(self, U, mask):\n",
    "        v_mask = torch.rand(self.V.size())\n",
    "        v_mask = torch.where(v_mask > 0.15, torch.full_like(v_mask, 1), torch.full_like(v_mask, 0))     # Remove cuda\n",
    "        self.V = nn.Parameter(self.V * v_mask)\n",
    "\n",
    "        results1 = torch.zeros(0).type(U.type())\n",
    "        results2 = torch.zeros(0).type(U.type())\n",
    "        h = torch.zeros((U.size(1), U.size(2))) # Removed cuda\n",
    "        c = torch.zeros((U.size(1), U.size(2))) # Removed cuda\n",
    "\n",
    "        for i in range(U.size()[0]):\n",
    "            if i == 0:\n",
    "                t_t = U[i]\n",
    "                v_cat = torch.cat((t_t, t_t), dim=1)\n",
    "                v_cat.size()\n",
    "                m_cat = v_cat.unsqueeze(1)\n",
    "                m_cat.size()\n",
    "                p = self.ac(m_cat.matmul(self.V).matmul(m_cat.transpose(1, 2)).contiguous().view(m_cat.size()[0], -1) + self.W(v_cat))\n",
    "                p = self.dropout(p)\n",
    "                h2 = self.conv(p.unsqueeze(0)).squeeze(0)\n",
    "                h, c = self.gru(p, (h, c))\n",
    "                h3 = self.cnn3(p.unsqueeze(0)).squeeze(0)\n",
    "                h_cat = torch.cat((h, h3), dim=1)\n",
    "                \n",
    "                h_cat = self.dropout(h_cat)\n",
    "                results1 = torch.cat((results1, h_cat))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                l_t = U[i-1]\n",
    "                t_t = U[i]\n",
    "                \n",
    "                v_cat = torch.cat((l_t, t_t), dim=1)\n",
    "                m_cat = v_cat.unsqueeze(1)\n",
    "                p = self.ac(m_cat.matmul(self.V).matmul(m_cat.transpose(1, 2)).contiguous().view(m_cat.size()[0], -1) + self.W(v_cat))\n",
    "                p = self.dropout(p)\n",
    "                h2 = self.conv(p.unsqueeze(0)).squeeze(0)\n",
    "                h, c = self.gru(p, (h, c))\n",
    "                h3 = self.cnn3(p.unsqueeze(0)).squeeze(0)\n",
    "                h_cat = torch.cat((h, h3), dim=1)\n",
    "                h_cat = self.dropout(h_cat)\n",
    "                results1 = torch.cat((results1, h_cat))\n",
    "                \n",
    "        \n",
    "        rever_U = self._reverse_seq(U, mask)\n",
    "\n",
    "        for i in range(rever_U.size()[0]):\n",
    "            if i == 0:\n",
    "                t_t = rever_U[i]\n",
    "                v_cat = torch.cat((t_t, t_t), dim=1)\n",
    "                m_cat = v_cat.unsqueeze(1)\n",
    "                p = self.ac(m_cat.matmul(self.V).matmul(m_cat.transpose(1, 2)).contiguous().view(m_cat.size()[0], -1) + self.W(\n",
    "                        v_cat))\n",
    "                p = self.dropout(p)\n",
    "                h2 = self.conv(p.unsqueeze(0)).squeeze(0)\n",
    "                h, c = self.gru(p, (h, c))\n",
    "                \n",
    "                h3 = self.cnn3(p.unsqueeze(0)).squeeze(0)\n",
    "                h_cat = torch.cat((h, h3), dim=1)\n",
    "                h_cat = self.dropout(h_cat)\n",
    "                results2 = torch.cat((results2, h_cat))\n",
    "            else:\n",
    "                \n",
    "                l_t = rever_U[i-1]\n",
    "                t_t = rever_U[i]\n",
    "                v_cat = torch.cat((l_t, t_t), dim=1)\n",
    "                m_cat = v_cat.unsqueeze(1)\n",
    "                p = self.ac(\n",
    "                    m_cat.matmul(self.V).matmul(m_cat.transpose(1, 2)).contiguous().view(m_cat.size()[0], -1) + self.W(\n",
    "                        v_cat))\n",
    "                p = self.dropout(p)\n",
    "                h2 = self.conv(p.unsqueeze(0)).squeeze(0)\n",
    "                h, c = self.gru(p, (h, c))\n",
    "\n",
    "\n",
    "                h3 = self.cnn3(p.unsqueeze(0)).squeeze(0)\n",
    "                h_cat = torch.cat((h, h3), dim=1)\n",
    "                h_cat = self.dropout(h_cat)\n",
    "                results2 = torch.cat((results2, h_cat))\n",
    "\n",
    "        results2 = results2.contiguous().view(rever_U.size(0), rever_U.size(1), -1)\n",
    "        results2 = self._reverse_seq(results2, mask)\n",
    "        results2 = results2.contiguous().view(results1.size(0), results1.size(1))\n",
    "        results = torch.log_softmax(self.Ws(torch.cat((results1, results2), dim=1)), dim=1)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, *input):\n",
    "        pass\n",
    "\n",
    "# Defining the training and evaluation model and returning parameters like average loss and average accuracy \n",
    "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
    "    losses = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    assert not train or optimizer != None\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    for data in dataloader:\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        textf, visuf, acouf, qmask, umask, label = [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
    "        log_prob = model(textf, umask)\n",
    "        labels_ = label.view(-1)  \n",
    "        loss = loss_function(log_prob, labels_, umask)\n",
    "        pred_ = torch.argmax(log_prob, 1)  \n",
    "        preds.append(pred_.data.cpu().numpy())\n",
    "        labels.append(labels_.data.cpu().numpy())\n",
    "        masks.append(umask.view(-1).cpu().numpy())\n",
    "\n",
    "        losses.append(loss.item() * masks[-1].sum())\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # print(torch.mean(model.V.grad))\n",
    "            optimizer.step()\n",
    "        #it += 1\n",
    "    if preds != []:\n",
    "        preds = np.concatenate(preds)\n",
    "        labels = np.concatenate(labels)\n",
    "        masks = np.concatenate(masks)\n",
    "    else:\n",
    "        return float('nan'), float('nan'), [], [], [], float('nan')\n",
    "\n",
    "    avg_loss = round(np.sum(losses) / np.sum(masks), 4)\n",
    "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks) * 100, 2)\n",
    "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted') * 100, 2)\n",
    "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--no-cuda] [--lr LR] [--l2 L2]\n",
      "                             [--rec-dropout rec_dropout] [--dropout dropout]\n",
      "                             [--batch-size BS] [--epochs E] [--class-weight]\n",
      "                             [--active-listener] [--attention ATTENTION]\n",
      "                             [--tensorboard]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/pritthishchattopadhyay/Library/Jupyter/runtime/kernel-04539953-244a-488d-9796-4d65bec13707.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the results using the parameters as defined below and seeing how the results vary.\n",
    "# All sections of the code have been implemented on Laptop CPU\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                        help='does not use GPU')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--l2', type=float, default=0.001, metavar='L2',\n",
    "                        help='L2 regularization weight')\n",
    "    parser.add_argument('--rec-dropout', type=float, default=0.1,\n",
    "                        metavar='rec_dropout', help='rec_dropout rate')\n",
    "    parser.add_argument('--dropout', type=float, default=0.0, metavar='dropout',\n",
    "                        help='dropout rate')\n",
    "    parser.add_argument('--batch-size', type=int, default=1, metavar='BS',\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='E',\n",
    "                        help='number of epochs')\n",
    "    parser.add_argument('--class-weight', action='store_true', default=True,\n",
    "                        help='class weight')\n",
    "    parser.add_argument('--active-listener', action='store_true', default=False,\n",
    "                        help='active listener')\n",
    "    parser.add_argument('--attention', default='concat', help='Attention type')\n",
    "    parser.add_argument('--tensorboard', action='store_true', default=False,\n",
    "                        help='Enables tensorboard log')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
    "    if args.cuda:\n",
    "        print('Running on GPU')\n",
    "    else:\n",
    "        print('Running on CPU')\n",
    "        \n",
    "    batch_size = args.batch_size\n",
    "    n_classes = 6\n",
    "    cuda = args.cuda\n",
    "    n_epochs = args.epochs\n",
    "    D_m = 100\n",
    "\n",
    "    model = RNTN(D_m, n_classes,'simple', True)\n",
    "    print('\\n number of parameters {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    loss_weights = torch.FloatTensor([\n",
    "                                        1/0.086747,\n",
    "                                        1/0.144406,\n",
    "                                        1/0.227883,\n",
    "                                        1/0.160585,\n",
    "                                        1/0.127711,\n",
    "                                        1/0.252668,\n",
    "                                        ])\n",
    "\n",
    "    if args.class_weight:\n",
    "        loss_function = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
    "    else:\n",
    "        loss_function = MaskedNLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=args.l2)\n",
    "\n",
    "    train_loader, valid_loader, test_loader =get_IEMOCAP_loaders(r'./IEMOCAP_features_raw.pkl',\n",
    "                                valid=0.0,\n",
    "                                batch_size=batch_size,\n",
    "                                num_workers=2)\n",
    "\n",
    "    best_loss, best_label, best_pred, best_mask,best_epoch = None, None, None, None,0\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc, _,_,_, train_fscore = train_or_eval_model(model, loss_function,train_loader, e, optimizer, True)\n",
    "        valid_loss, valid_acc, _,_,_, val_fscore = train_or_eval_model(model, loss_function, valid_loader, e)\n",
    "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore = train_or_eval_model(model, loss_function, test_loader, e)\n",
    "\n",
    "        if best_loss == None or best_loss > test_loss:\n",
    "            best_loss, best_label, best_pred, best_mask =test_loss, test_label, test_pred, test_mask\n",
    "            best_epoch= e\n",
    "        patience=10\n",
    "        if(e - best_epoch >= patience):\n",
    "            break\n",
    "        print('epoch {} train_loss {} train_acc {} train_fscore{} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\n",
    "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n",
    "\n",
    "\n",
    "\n",
    "    print('Test performance..')\n",
    "    print('Loss {}  Best_Epoch {} accuracy {}'.format(best_loss, best_epoch+1,\n",
    "                                     round(accuracy_score(best_label, best_pred, sample_weight=best_mask)*100,2)))\n",
    "    print(classification_report(best_label,best_pred,sample_weight=best_mask, digits=4))\n",
    "    print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
